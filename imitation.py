import tensorflow as tf
import keras
import numpy as np

from model import make_model

def action_to_one_hot(env, action):
    action_vec = np.zeros(env.action_space.n)
    action_vec[action] = 1
    return action_vec    

def generate_episode(env, policy):
    """Collects one rollout from the policy in an environment. The environment
    should implement the OpenAI Gym interface. A rollout ends when done=True. The
    number of states and actions should be the same, so you should not include
    the final state when done=True.

    Args:
    env: an OpenAI Gym environment.
    policy: a keras model
    Returns:
    states: a list of states visited by the agent.
    actions: a list of actions taken by the agent. While the original actions
        are discrete, it will be helpful to use a one-hot encoding. The actions
        that you return should be one-hot vectors (use action_to_one_hot())
    rewards: the reward received by the agent at each step.
    """
    done = False
    state = env.reset()
#     print("STATE ", state)
#     print("State shape ", np.expand_dims(state,0).shape)
#     print("action space ", env.action_space)
    states = [state]
    actions = []
    rewards = []
    
    while not done:
        # WRITE CODE HERE
        action = np.argmax(policy(np.expand_dims(state,0)))
        # print("action shape ", action)
        state, reward, done,_ = env.step(action)
        if not done:
            states.append(state)
        actions.append(action_to_one_hot(env,action))
        rewards.append(reward)
        
    return states, actions, rewards

class Imitation():
    
    def __init__(self, env, num_episodes, expert_file):
        self.env = env
        self.expert = tf.keras.models.load_model(expert_file)
        self.num_episodes = num_episodes
        print("Expert insput shape ", self.expert.input_shape)
        self.model = make_model()
        print("Model input shape ", self.model.input_shape)
        
    def generate_behavior_cloning_data(self):
        self._train_states = []
        self._train_actions = []
        for _ in range(self.num_episodes):
            states, actions, _ = generate_episode(self.env, self.expert)
            self._train_states.extend(states)
            self._train_actions.extend(actions)
            # self._train_states = numpy.concatenate(self._train_states , states)
            # self._train_actions = numpy.concatenate(self._train_actions , actions)

        self._train_states = np.array(self._train_states)
        self._train_actions = np.array(self._train_actions)
        
    def generate_dagger_data(self):
        # WRITE CODE HERE
        # You should collect states and actions from the student policy
        # (self.model), and then relabel the actions using the expert policy.
        # This method does not return anything.
        # END
        self._train_states = []
        self._train_actions = []
        for _ in range(self.num_episodes):
            states, actions, _ = generate_episode(self.env, self.model)
            self._train_states.extend(states)
        
        for state in self._train_states:
            action = np.argmax(self.expert(np.expand_dims(state,0)))
            
            self._train_actions.append(action_to_one_hot(self.env,action))
#         print("Actions ", self_train_actions.shape)
        self._train_states = np.array(self._train_states)
        self._train_actions = np.array(self._train_actions)
        
    def train(self, num_epochs=1):
        """Trains the model on training data generated by the expert policy.
        Args:
            num_epochs: number of epochs to train on the data generated by the expert.
            NOTE: Oct 8 1pm: change num_epochs to 1 by instructors
        Return:
            loss: (float) final loss of the trained policy.
            acc: (float) final accuracy of the trained policy
        """
#         print("Train states", self._train_states.shape)
#         print("Train action", self._train_actions.shape)
        hist = self.model.fit(self._train_states,
                              self._train_actions,
                              epochs=num_epochs, 
                              batch_size = 64)
        
        return hist.history['loss'], hist.history['accuracy']
            
        
        # WRITE CODE HERE
        # END


    def evaluate(self, policy, n_episodes=50):
        rewards = []
        for i in range(n_episodes):
            _, _, r = generate_episode(self.env, policy)
            rewards.append(sum(r))
        r_mean = np.mean(rewards)
        return r_mean
    