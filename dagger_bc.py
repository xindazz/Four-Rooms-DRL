# -*- coding: utf-8 -*-
"""DAGGER & BC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_IxkNMp8UL2Gga9uSoSrLUCHx4s3-Vgu
"""

from collections import OrderedDict 
import gym
import matplotlib.pyplot as plt
import numpy as np
import random
import time

from imitation import Imitation
	
	

def generate_imitation_results(mode, expert_file, keys=[100], num_seeds=1, num_iterations=100):
    # Number of training iterations. Use a small number
    # (e.g., 10) for debugging, and then try a larger number
    # (e.g., 100).

    # Dictionary mapping number of expert trajectories to a list of rewards.
    # Each is the result of running with a different random seed.
    reward_data = OrderedDict({key: [] for key in keys})
    accuracy_data = OrderedDict({key: [] for key in keys})
    loss_data = OrderedDict({key: [] for key in keys})

    for num_episodes in keys:
        for t in range(num_seeds):
            print('*' * 50)
            print('num_episodes: %s; seed: %d' % (num_episodes, t))

            # Create the environment.
            env = gym.make('CartPole-v0')
            env.seed(t) # set seed
            im = Imitation(env, num_episodes, expert_file)
            expert_reward = im.evaluate(im.expert)
            print('Expert reward: %.2f' % expert_reward)
                
            loss_vec = []
            acc_vec = []
            imitation_reward_vec = []
            # print("Training data", im._train_states.shape)
            print("MODE ", mode)
            for i in range(num_iterations):
                print("EPOCH {%d}" % i)
                # WRITE CODE HERE
                if (mode == 'behavior cloning'):
                  print("Getting BC data")
                  im.generate_behavior_cloning_data()
                else:
                  im.generate_dagger_data()
                print("Training data", im._train_states.shape)
                loss, acc = im.train()
                loss_vec.append(loss)
                acc_vec.append(acc)
                reward = im.evaluate(im.model)
                imitation_reward_vec.append(reward)
            
#             print("Imitation reward shape ", imitation_reward_vec.shape)
            reward_data[num_episodes].append(imitation_reward_vec)
            accuracy_data[num_episodes].append(acc_vec)
            loss_data[num_episodes].append(loss_vec)
    
    # END
    
        reward_data[num_episodes] = np.mean(np.array(reward_data[num_episodes]), axis=0)
        accuracy_data[num_episodes] = np.mean(np.array(accuracy_data[num_episodes]), axis=0)*100
        loss_data[num_episodes] = np.mean(np.array(loss_data[num_episodes]), axis=0)
        
    return reward_data, accuracy_data, loss_data, expert_reward


"""### Experiment: Student vs Expert
In the next two cells, you will compare the performance of the expert policy
to the imitation policies obtained via behavior cloning and DAGGER.
"""
def plot_student_vs_expert(mode, expert_file, keys=[100], num_seeds=1, num_iterations=100):
    assert len(keys) == 1
    reward_data, acc_data, loss_data, expert_reward = \
        generate_imitation_results(mode, expert_file, keys, num_seeds, num_iterations)

    ### Plot the results
    plt.figure(figsize=(12, 3))
    plt.plot(reward_data[keys[0]], label='reward' +str(keys[0]))
    plt.plot([expert_reward]*len(reward_data[keys[0]]),label='expert reward')
    plt.legend()  
    plt.savefig('reward_p1_expert_data_%s.png' % mode, dpi=300)
    plt.show()
    
    plt.figure(figsize=(12, 4))
    plt.plot(acc_data[keys[0]], label='accuracy'+str(keys[0]))
    plt.legend()  
    plt.savefig('acc_p1_expert_data_%s.png' % mode, dpi=300)
    plt.show()

    plt.figure(figsize=(12, 4))
    plt.plot(loss_data[keys[0]], label='loss'+str(keys[0]))
    plt.legend()  
    plt.savefig('loss_p1_expert_data_%s.png' % mode, dpi=300)
    plt.show()

"""Plot the reward, loss, and accuracy for each, remembering to label each line."""
def plot_compare_num_episodes(mode, expert_file, keys, num_seeds=1, num_iterations=100):
    s0 = time.time()
    reward_data, accuracy_data, loss_data, _ = \
        generate_imitation_results(mode, expert_file, keys, num_seeds, num_iterations)
    
    ### Plot the results
    plt.figure(figsize=(12, 4))
    # WRITE CODE HERE
    for i in range(len(keys)):
      plt.plot(reward_data[keys[i]], label='reward' +str(keys[i]))
    
    plt.plot([200.0]*len(reward_data[keys[0]]),label='expert reward')
    plt.legend()  
    plt.savefig('comp_reward_p1_expert_data_%s.png' % mode, dpi=300)
    plt.show()

    plt.figure(figsize=(12, 4))
    for i in range(len(keys)):
      plt.plot(accuracy_data[keys[i]], label='accuracy'+str(keys[i]))
    plt.legend()  
    plt.savefig('comp_acc_p1_expert_data_%s.png' % mode, dpi=300)
    plt.show()

    plt.figure(figsize=(12, 4))
    for i in range(len(keys)):
      plt.plot(loss_data[keys[i]], label='loss'+str(keys[i]))
    plt.legend()  
    plt.savefig('comp_loss_p1_expert_data_%s.png' % mode, dpi=300)
    plt.show()
    
    # END
    
    # plt.show()
    print('time cost', time.time() - s0)


def main():
    # generate all plots for Problem 1
    expert_file = 'expert.h5'
    
    # switch mode
    mode = 'behavior cloning'
    # mode = 'dagger'

    # change the list of num_episodes below for testing and different tasks
    keys = [100] #[1, 10, 50, 100] 
    num_seeds = 1 # 3
    num_iterations = 100  # Number of training iterations. Use a small number
                            # (e.g., 10) for debugging, and then try a larger number
                            # (e.g., 100).

    # Q1.1.1, Q1.2.1
    # plot_student_vs_expert(mode, expert_file, keys, num_seeds=num_seeds, num_iterations=num_iterations)

    keys = [1, 10, 50, 100] 
    num_seeds = 3
    # Q1.1.2, Q1.2.2
    plot_compare_num_episodes(mode, expert_file, keys, num_seeds=num_seeds, num_iterations=num_iterations)
    

if __name__ == '__main__':
    main()